{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: Wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This report summarises the work done in downloading and wrangling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "There were three main sources of the data as part of wrangling efforts. These were:\n",
    "1. A `twitter_archive_enhanced.csv` file which was provided directly by Udacity and downloaded directly into the working environment. This file was then read into a dataframe using the pd.read_csv function\n",
    "2. An `image predictions.csv` file which is hosted on Udacity's servers and was downloaded programmatically using the Requests library. The file was subsequently read into a dataframe using the pd.read_csv function and a line separator\n",
    "3. Twitter, using Python's Tweepy library to query the Twitter API for each tweet's retweet count and favorite (\"like\") count at the minimum. The tweet IDs in the `twitter_archive_enhanced.csv` file were used for the query and the results were stored in a `tweet.json.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessing\n",
    "\n",
    "#### Approach\n",
    "\n",
    "The data gathered was assessed both visually and programmatically using key python functions such as:\n",
    "* .head()\n",
    "* .info()\n",
    "* .duplicated()\n",
    "* .sample()\n",
    "* .value_counts()\n",
    "* .sort_values()\n",
    "\n",
    "Issues identified were classified into `Quality` and `Tidiness` Issues. \n",
    "* `Quality` Issues stem from data that has issues with data content including missing data, invalid data, inaccurate date or inconsistent data\n",
    "* `Tidiness` Issues come from data that has issues with its structure (columns, rows or table)\n",
    "\n",
    "#### Issues Identified\n",
    "The following issues were identified as part of the assessing phase:\n",
    "\n",
    "\n",
    "#### Quality issues\n",
    "###### The twitter archive dataframe\n",
    "\n",
    "1.There are 181 retweets. Retweets are often tweets of other tweets and so this may imply duplicate data. For the purpose of this analysis, only original tweets will be used.\n",
    "\n",
    "2.The timestamp column is a string and should rather be in a timestamp format.\n",
    "\n",
    "3.**None** is repeated for most of the dog names instead of NaN for null fields\n",
    "\n",
    "4.There are inaccurate dog names such as **a, an** and **the**. We also notice that these inaccurate names begin with small letters\n",
    "\n",
    "5.The source of the tweets are in the form of URLs. This can make it difficult to perform any analysis by the Tweet source\n",
    "\n",
    "\n",
    "#### The image prediction dataframe\n",
    "6.Inconsistent upper and lower cases in the **p1**, **p2** and **p3** columns\n",
    "\n",
    "7.Underscore is used instead of spaces for the dog breed predictions in the **p1**, **p2** and **p3** columns\n",
    "\n",
    "8.False predictions in all three columns, **p1**, **p2** and **p3**. This can imply that these tweet IDs are not dogs at all\n",
    "\n",
    "\n",
    "#### Tidiness Issues\n",
    "###### The twitter archive dataframe\n",
    "1.The **jpg_url** column should be part of the twitter_archives dataframe\n",
    "\n",
    "2.The **favourite_count** and **retweet_count** columns should be part of the twitter_archives dataframe\n",
    "\n",
    "3.The predicted dog breed should be selected and merged with the twitter_archives dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Copies of the original dataframes were made for the purpose of cleaning. \n",
    "\n",
    "The data cleaning involved many process including but not limited to converting datatypes of columns, ensuring consistency in naming, deleting columns and rows where necessary, and merging dataframes. The cleaning process used the define-code-test framework. \n",
    "\n",
    "For each issue identified, the issue was defined, code was written to resolve the issue and the results of the code writted was tested to ascertain if the issue had been resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
